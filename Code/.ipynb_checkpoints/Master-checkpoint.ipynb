{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Loan Defaults\n",
    "> Author: Alex Lau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Table of Contents](#1.-Table-of-Contents)\n",
    "2. [Exploratory Data Analysis](#2.-Exploratory-Data-Analysis)\n",
    "    <br>2.1 [Import Packages and Data](#2.1-Import-Packages-and-Data)\n",
    "    <br>2.2 [High Level Checks](#2.2-High-Level-Checks)\n",
    "    <br>2.3 [Investigating Target Variable](#2.3-Investigating-Target-Variable)\n",
    "    <br>2.4 [Investigating Features](#2.4-Investigating-Features)\n",
    "3. [Data Cleaning](#3.-Data-Cleaning)\n",
    "4. [Feature Engineering](#4.-Feature-Engineering)\n",
    "5. [Revisiting Exploratory Data Analysis: Correlations Deep Dive](#5.-Revisiting-Exploratory-Data-Analysis:-Correlations-Deep-Dive)\n",
    "6. [Preprocessing](#6.-Preprocessing)\n",
    "7. [Modeling](#7.-Modeling)\n",
    "    <br>7.1 [Baseline Model](#7.1-Baseline-Model)\n",
    "    <br>7.2 [Logistic Regression](#7.2-Logistic-Regression)\n",
    "    <br>7.3 [KNeighbors Classifier](#7.3-KNeighbors-Classifier)\n",
    "    <br>7.4 [Random Forest Classifier](#7.4-Random-Forest-Classifier)\n",
    "    <br>7.5 [Extra Trees Classifier](#7.5-Extra-Trees-Classifier)\n",
    "    <br>7.6 [AdaBoost Classifier](#7.6-AdaBoost-Classifier)\n",
    "    <br>7.7 [Support Vector Machine](#7.7-Support-Vector-Machine)\n",
    "    <br>7.8 [Gaussian Naive Bayes Classifier](#7.8-Gaussian-Naive-Bayes-Classifier)\n",
    "    <br>7.9 [Gradient Boost Classifier](#7.9-Gradient-Boost-Classifier)\n",
    "    <br>7.10 [Voting Classifier](#Voting-Classifier)\n",
    "8. [Conclusions and Evaluation](#8.-Conclusions-and-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from this website after creating an account\n",
    "https://www.lendingclub.com/info/statistics.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in main dataframe that we created from the \"Downloading Data\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the main dataframe we have created after consolidating the downloaded files.\n",
    "# this results in a mixed type warning, so we will check what these columns are in the next section\n",
    "df = pd.read_csv('../Data/FullLoanStats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 High Level Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the first 5 rows of our dataframe\n",
    "# we will remove the Unnamed columns, but first we will check the warning on mixed type columns since these are indexed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns will be removed for our model, since these are not relevant for predicting a loan default before approving the loan. Deb settlement refers to activity post default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# columns 51, 146, 147, 148 have mixed types\n",
    "\n",
    "# create a list of these columns\n",
    "mixed_type_columns = [51, 146, 147, 148]\n",
    "\n",
    "# iterate through this list and print every column name\n",
    "for column in mixed_type_columns:\n",
    "    print(list(df.columns)[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnamed columns\n",
    "df.drop(columns = ['Unnamed: 0', 'Unnamed: 1'], inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking number of rows and columns of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing view limitation so we can see all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# checking the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking datatypes, memory usage, \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting row view limitation, 150 for the number of columns\n",
    "pd.set_option(\"display.max_rows\", 150)\n",
    "# viewing datatypes of each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the counts of the null values of our dataset. We will remove all features where all values are the same. This includes where null values = 44171, since that is the shape of our dataframe. We will also remove all data that is collected after the loan is approved, includuing settlement columns mentioned earlier, since we are trying to predict loan defaults from before the loan is approved. This activity will be reviewed in section 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing all columns with null values sorted by count\n",
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Investigating Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting distribution of target variable\n",
    "df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the quantities\n",
    "df['loan_status'].value_counts().plot.bar()\n",
    "plt.title('Loan Status Counts', size = 20)\n",
    "plt.xlabel('Categories', size = 15)\n",
    "plt.ylabel('Quantity', size = 15)\n",
    "plt.xticks(rotation = 0, size = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting percentages of each value\n",
    "df['loan_status'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly over 12% of our list of loans were defaulted, which represents heavy imbalance and will cause poor model performance. This is expected to be a low percentage, since Lending Club would not approve loans if they thought the borrowers would default. They have also been improving on the fully paid vs charged off or default ratio over the years. \n",
    "We will use Smote to help with our unbalanced classes during the preprocessing sections.\n",
    "\n",
    "The term \"Charged off\" is when a creditor, Lending Club in this case, gives up hope you will repay the money after months of not paying the mininum payments and writes off the debt. Learn more about it in this article from Marketwatch. \n",
    "https://www.marketwatch.com/story/everything-you-need-to-know-about-a-charged-off-debt-2019-08-15\n",
    "\n",
    "We will convert Fully Paid values to 0 and defaulted/charged off to 1 for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# investigating the mean values of the numberical features for each target variable group\n",
    "df.groupby(by = ['loan_status']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Investigating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for returning the unique values and count, sorted by descending order, based on user specified column\n",
    "def show_values(column):\n",
    "    return df[column].value_counts().sort_values(ascending = False)\n",
    " \n",
    "# function for counting null values in a specified column\n",
    "def null_count(column):\n",
    "    return df[column].isnull().value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking that all id values are unique no repeats\n",
    "show_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check distribution of applications\n",
    "show_values('application_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that average loans approved for Joint applications are over 19,000 USD, where individuals are 14,000 USD. Joint apps also have a higher joint debt to income ratios at 18.5 vs 17.5 for individuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking any visible differences in values of features joint vs individual applications\n",
    "df.groupby(by = ['application_type']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting the percentages of loan statuses based on individual applications\n",
    "df[df['application_type'] == 'Individual']['loan_status'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On average joint applications have higher probability of defaulting or being charged off. \n",
    "df[df['application_type'] == 'Joint App']['loan_status'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to treat individual and joint applications differently, since several features for joint applications are different than individual applications, such as the loan values, incomes, loan repayment rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating separate dataframes for individual and joint apps\n",
    "df_individual = df[df['application_type']=='Individual']\n",
    "df_JointApp = df[df['application_type']=='Joint App']\n",
    "\n",
    "# checking individual dataframes\n",
    "df_individual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking joint dataframe\n",
    "df_JointApp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How many emp_titles are there? We will drop this since there are over 18,000 unique values\n",
    "show_values('emp_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking values within payment plan, perhaps we can drop these since they are all the same\n",
    "show_values('pymnt_plan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we confirm there are no null values in payment plan, so we are good to drop\n",
    "null_count('pymnt_plan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking values within feature purpose\n",
    "show_values('purpose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the count of these values is identical to feature 'purpose' so we will drop one. \n",
    "# We will drop title because historical files have null values for in this feature.\n",
    "show_values('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# categories of emp length, we can get_dummies, but we'll first need to check null values in the next cell\n",
    "show_values('emp_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null counts, we will replace these values with 'Unknown'\n",
    "null_count('emp_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts of values for public records, convert to int\n",
    "show_values('pub_rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts of values for public records\n",
    "show_values('initial_list_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# policy codes are all the same values, remove this column\n",
    "show_values('policy_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hardship flag, they are all the same values, we will remove this feature since it won't help our model\n",
    "show_values('hardship_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how code the df['loan_status'] values of these debt_settlement_flags?\n",
    "show_values('debt_settlement_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# revol_bal_joint\n",
    "show_values('revol_bal_joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what are all of the unique values in the grade feature? We will build a dictionary later to convert these to numbers\n",
    "show_values('grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique values for sub_grade, these will also be a dictionary\n",
    "show_values('sub_grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are no null values in this column within the joint applications. \n",
    "# We are safe to impute 0 for the null values in this column for the larger dataframe for individual applications\n",
    "df_JointApp['revol_bal_joint'].isnull().value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38812 null values refer to the individual applications\n",
    "null_count('revol_bal_joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can also impute 0 for individual accounts for these values as well.\n",
    "df_JointApp['sec_app_fico_range_low'].isnull().value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing null values for this column on the larger dataframe\n",
    "null_count('sec_app_fico_range_low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can also impute 0 for individual accounts for these values as well.\n",
    "df_JointApp['sec_app_earliest_cr_line'].isnull().value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing null values for this column on the larger dataframe\n",
    "null_count('sec_app_earliest_cr_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can also impute 0 for individual accounts for these values as well.\n",
    "df_JointApp['annual_inc_joint'].isnull().value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing null values for this column on the larger dataframe\n",
    "null_count('annual_inc_joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also impute 0 for individual accounts for these values as well.\n",
    "df_JointApp['dti_joint'].isnull().value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing null values for this column on the larger dataframe\n",
    "null_count('dti_joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the values in verification_status_joint column\n",
    "show_values('verification_status_joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# There are over 444 null values for verification statuses on joint applications. We will impute \"Unknown\" for these. \n",
    "df_JointApp['verification_status_joint'].isnull().value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will impute \"N/A\" for these values in individual accounts\n",
    "df_individual['verification_status_joint'].isnull().value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tl_120dpd_2m seems unhelpful\n",
    "show_values('num_tl_120dpd_2m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null count for this feature\n",
    "null_count('num_tl_120dpd_2m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data types for percentages, all values in interest rates are strings\n",
    "df[df['int_rate'].map(type) != str].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately not all values in revolving utilization are strings, they are a mix of strings and ints\n",
    "df[df['revol_util'].map(type) != str].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the values in question in revolving utilization column. We need to convert this column into the same type\n",
    "df[df['revol_util'].map(type) != str]['revol_util']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a correlation chart at some point, either before or after cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for Y target variable\n",
    "loan_status_dict = {'Fully Paid':0, 'Charged Off':1, 'Default':1}\n",
    "\n",
    "df['loan_status'].replace(loan_status_dict, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unnecessary features for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of columns to remove\n",
    "remove_cols = ['member_id', 'funded_amnt', 'emp_title', 'issue_d', 'pymnt_plan', 'url', 'desc', 'title', \n",
    "               'zip_code', 'earliest_cr_line', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', \n",
    "               'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', \n",
    "               'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d', 'last_fico_range_high', \n",
    "               'last_fico_range_low', 'policy_code', 'sec_app_earliest_cr_line', 'num_tl_120dpd_2m', \n",
    "               'hardship_payoff_balance_amount', 'hardship_type', 'payment_plan_start_date', 'hardship_length', \n",
    "               'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', 'hardship_amount',\n",
    "               'hardship_last_payment_amount', 'deferral_term', 'hardship_start_date', 'hardship_reason', \n",
    "               'hardship_status', 'hardship_end_date', 'settlement_term', 'settlement_percentage', 'settlement_amount',\n",
    "               'settlement_date', 'settlement_status', 'debt_settlement_flag_date', 'hardship_flag', \n",
    "               'debt_settlement_flag']\n",
    "\n",
    "# removing the unnecessary columns\n",
    "df.drop(columns = remove_cols, inplace = True)\n",
    "\n",
    "# viewing the new shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create columns for conditional features where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns we want to create conditional columns to represent whether a value is populated\n",
    "# Later we will replace null values in the original columns, so having this extra conditional column will help the model\n",
    "conditional_columns = ['sec_app_mths_since_last_major_derog', 'mths_since_recent_bc_dlq', 'mths_since_last_major_derog',\n",
    "                       'mths_since_recent_revol_delinq', 'mths_since_last_delinq', 'il_util', 'mths_since_recent_inq',\n",
    "                       'mo_sin_old_il_acct', 'bc_util', 'percent_bc_gt_75', 'bc_open_to_buy', 'mths_since_recent_bc',\n",
    "                       'revol_util', 'all_util', 'avg_cur_bal', 'sec_app_revol_util'] \n",
    "\n",
    "# function to create new columns\n",
    "def create_conditionals(df, columns):\n",
    "    \n",
    "    # iterate through the list of columns\n",
    "    for column in columns:\n",
    "        \n",
    "        # creating a new column name\n",
    "        new_name = column + '_conditional'\n",
    "        \n",
    "        # values are tranformed into 0 is null, 1 if there is a value\n",
    "        df[new_name] = df[column].isnull().map({False:1, True: 0})\n",
    "        \n",
    "    return df\n",
    "\n",
    "# calling this function now\n",
    "create_conditionals(df, conditional_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the new number of columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of columns that we will fill in null values for \n",
    "individual_null_to_NA = ['verification_status_joint']\n",
    "joint_null_to_unknown = ['verification_status_joint']\n",
    "\n",
    "null_to_unknown = ['emp_length']\n",
    "null_to_zero = ['sec_app_mths_since_last_major_derog', 'annual_inc_joint', 'revol_bal_joint', \n",
    "                     'sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_mort_acc', 'sec_app_inq_last_6mths',\n",
    "                    'sec_app_open_acc', 'sec_app_open_act_il', 'sec_app_num_rev_accts', 'sec_app_chargeoff_within_12_mths',\n",
    "                    'sec_app_collections_12_mths_ex_med', 'dti_joint', 'mths_since_last_record', 'mths_since_recent_bc_dlq',\n",
    "                    'mths_since_last_major_derog', 'mths_since_recent_revol_delinq', 'mths_since_last_delinq', 'il_util', \n",
    "                    'mths_since_recent_inq', 'mo_sin_old_il_acct', 'mths_since_rcnt_il', 'bc_util', 'percent_bc_gt_75', \n",
    "                    'bc_open_to_buy', 'mths_since_recent_bc', 'revol_util', 'all_util', 'avg_cur_bal', 'sec_app_revol_util']\n",
    "null_to_max = ['dti']\n",
    "\n",
    "# function to fill in the missing values\n",
    "def replace_null(df, columns, value):\n",
    "    for column in columns:\n",
    "        df[column].fillna(value, inplace = True)\n",
    "    return df\n",
    "\n",
    "# calling replace null function on the list of columns we will convert to unknown\n",
    "replace_null(df, null_to_unknown, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing, this works! \n",
    "show_values('emp_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing null values for columns in null_to_zero list\n",
    "replace_null(df, null_to_zero, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing null values in column dti (debt to income) with max value, which we know is 999\n",
    "replace_null(df, null_to_max, 999) # we already know the max value is 999 from the describe function during EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# There are 39256 null values for verification status joint\n",
    "null_count('verification_status_joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38812 of these values are associated to individual applications\n",
    "df.loc[df.application_type =='Individual','verification_status_joint'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The remaining 444 null values are associated to Joint applications\n",
    "df.loc[df.application_type =='Joint App','verification_status_joint'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will now update verification status joint features for individual applications from null to \"Not Applicable\"\n",
    "df.loc[df.application_type=='Individual','verification_status_joint'] = df.loc[df.application_type =='Individual','verification_status_joint'].fillna('Not Applicable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now update verification status joint values for joint applications from null to \"Unknown\"\n",
    "df.loc[df.application_type =='Joint App','verification_status_joint'] = df.loc[df.application_type =='Joint App','verification_status_joint'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking null count for this feature, there should no longer be null values in this column\n",
    "null_count('verification_status_joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if we have any null values for all features, we do not\n",
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Strings into Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionaries for ordinal values\n",
    "grade_dict = {'A': 1,'B': 2,'C': 3,'D': 4, 'E':5, 'F': 6, 'G':7}\n",
    "sub_grade_dict = {'A1':1, 'A2':2, 'A3':3, 'A4':4, 'A5':5, \n",
    "                  'B1':6, 'B2':7, 'B3':8, 'B4':9, 'B5':10,\n",
    "                  'C1':11, 'C2':12, 'C3':13, 'C4':14, 'C5':15,\n",
    "                  'D1':16, 'D2':17, 'D3':18, 'D4':19, 'D5':20,\n",
    "                  'E1':21, 'E2':22, 'E3':23, 'E4':24, 'E5':25, \n",
    "                  'F1':26, 'F2':27, 'F3':28, 'F4':29, 'F5':30, \n",
    "                  'G1':31, 'G2':32, 'G3':33, 'G4':34, 'G5':35}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the values from the dictionary\n",
    "df['grade'].replace(grade_dict, inplace = True)\n",
    "df['sub_grade'].replace(sub_grade_dict, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking the changes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Perecentages to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to convert column 'revol_util' into strings since this had mixed data types\n",
    "df['revol_util'] = df['revol_util'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of columns that have %s\n",
    "percent_list = ['int_rate', 'revol_util']\n",
    "\n",
    "# creating function to strip the % from each row of the specified columns, and then convert the values to float type\n",
    "def convert_percent_to_num(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].map(lambda x: x.rstrip('%'))\n",
    "        # convert the string to float\n",
    "        df[column] = df[column].astype('float64')\n",
    "    return df\n",
    "\n",
    "convert_percent_to_num(df, percent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting out categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of Categorical features\n",
    "categories = ['term', 'emp_length', 'home_ownership', 'verification_status', 'purpose', 'addr_state', \n",
    "              'initial_list_status', 'application_type', 'verification_status_joint']\n",
    "\n",
    "# separate columns for all categorical features\n",
    "df = pd.get_dummies(data = df, columns = categories, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting index to id number\n",
    "df.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking new shape of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# displaying all rows so we can see every column\n",
    "pd.set_option(\"display.max_rows\", 198)\n",
    "# checking all datatypes are numerical values, they are\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider reorganizing the imputing and cleaning sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Revisiting Exploratory Data Analysis: Correlations Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlation all the features\n",
    "plt.figure(figsize = (6,100))\n",
    "sns.heatmap(df.corr()[['loan_status']].sort_values(by = 'loan_status', ascending = False), annot = True, cmap = 'RdBu')\n",
    "plt.title('Features Correlation to Loan Payback', fontsize = 15)\n",
    "plt.xlabel('Loan Fully Paid Correlation', fontsize = 15)\n",
    "plt.ylabel('Features', fontsize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin modeling, we need to identify our X features and Y target variables for our models. We need to split the data into training and testing set, in this thase we will use the default 80/20 split. Scaling the data is necessary because we have a large number of features with a variety of ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "X = df.drop(columns = 'loan_status')\n",
    "y = df['loan_status']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)\n",
    "\n",
    "# Scaling data\n",
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how imbalanced our classes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_fully_paid = pd.Series(y_train).value_counts().sort_values(ascending = False)[0]\n",
    "y_train_charged_off = pd.Series(y_train).value_counts().sort_values(ascending = False)[1]\n",
    "\n",
    "# observe that data has been balanced in training data\n",
    "pd.Series(y_train).value_counts().sort_values(ascending = False).plot.bar(color = ['steelblue', 'orange'])\n",
    "plt.title('Training Data Values Pre Balancing', size = 20)\n",
    "plt.ylabel('Counts', size = 15)\n",
    "plt.xlabel('Loan Status', size = 15, rotation = 0)\n",
    "plt.xticks(rotation = 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# instantiating SMOTE\n",
    "sm = SMOTE(random_state = 42)\n",
    "# fitting the scaled train data\n",
    "X_train_new, y_train_new = sm.fit_sample(X_train_sc, y_train.ravel())\n",
    "\n",
    "# checking the new counts\n",
    "print(\"Number transactions after balancing X_train dataset: \", X_train_new.shape)\n",
    "print(\"Number transactions after balancing y_train dataset: \", y_train_new.shape)\n",
    "\n",
    "# observe that data has been balanced in training data\n",
    "pd.Series(y_train_new).value_counts().sort_values(ascending = False).plot.bar(color = ['steelblue', 'orange'])\n",
    "plt.title('Training Data Values Post Balancing', size = 20)\n",
    "plt.ylabel('Counts', size = 15)\n",
    "plt.xlabel('Loan Status', size = 15)\n",
    "plt.xticks(rotation = 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline Recall/Sensitivity score results in 0 when we are predicting all cases as the majority class, all borrowers fully pay back their loans. We will also aim to beat the baseline accuracy score of 87.55%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calcuating the baseline model for accuracy\n",
    "1 - y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression results in a test recall score of 61.24% but at the cost of our accuracy, which was 67.12%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate model for logistic regression, including max iterations\n",
    "lr = LogisticRegression(random_state = 42, max_iter = 500)\n",
    "\n",
    "# Fit model\n",
    "lr.fit(X_train_new, y_train_new)\n",
    "\n",
    "# checking accuracy scores\n",
    "print(f'Logistic Regression Train Accuracy: {lr.score(X_train_new, y_train_new)}')\n",
    "print(f'Logistic Regression Test Accuracy: {lr.score(X_test_sc, y_test)}')\n",
    "\n",
    "# predicting values for linear regression\n",
    "y_hat_lr_train = lr.predict(X_train_new)\n",
    "y_hat_lr_test = lr.predict(X_test_sc)\n",
    "\n",
    "# checking recall scores\n",
    "lr_recall_train = recall_score(y_train_new, y_hat_lr_train)\n",
    "lr_recall_test = recall_score(y_test, y_hat_lr_test)\n",
    "\n",
    "print(f'Logistic Regression Train Recall: {lr_recall_train}')\n",
    "print(f'Logistic Regression Test Recall: {lr_recall_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to run models and print Train and Test accuracy and recall scores for all other models\n",
    "def run_model(model_name):\n",
    "    # instantiating the model\n",
    "    model = model_name()\n",
    "    # Fit model\n",
    "    model.fit(X_train_new, y_train_new)\n",
    "    \n",
    "    # checking accuracy scores\n",
    "    print(f'{model_name} Train Accuracy: {model.score(X_train_new, y_train_new)}')\n",
    "    print(f'{model_name} Test Accuracy: {model.score(X_test_sc, y_test)}')\n",
    "\n",
    "    # predicting values for linear regression\n",
    "    y_hat_train = model.predict(X_train_new)\n",
    "    y_hat_test = model.predict(X_test_sc)\n",
    "\n",
    "    # checking recall scores\n",
    "    recall_train = recall_score(y_train_new, y_hat_train)\n",
    "    recall_test = recall_score(y_test, y_hat_test)\n",
    "\n",
    "    print(f'{model_name} Train Recall: {recall_train}')\n",
    "    print(f'{model_name} Test Recall: {recall_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest classifier results in a poor testing recall score. This seems as if we simply predicted the majority class, since we have a similar accuracy score to the baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Trees also performed poorly in recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(ExtraTreesClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost test recall score performs slightly better than random forest and extra trees but still very low at 7.27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(AdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate support vector machine.\n",
    "svc = SVC(gamma=\"scale\")\n",
    "\n",
    "# Fit support vector machine to training data.\n",
    "svc.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Score model\n",
    "print(f'Accuracy Score on training set: {svc.score(X_train_new, y_train_new)}')\n",
    "print(f'Accuracy Score on testing set: {svc.score(X_test_sc, y_test)}')\n",
    "\n",
    "# predicting values for linear regression\n",
    "y_hat_svc_train = svc.predict(X_train_new)\n",
    "y_hat_svc_test = svc.predict(X_test_sc)\n",
    "\n",
    "# checking recall scores\n",
    "svc_recall_train = recall_score(y_train_new, y_hat_svc_train)\n",
    "svc_recall_test = recall_score(y_test, y_hat_svc_test)\n",
    "\n",
    "print(f'Suppor Vector Machine Train Recall: {svc_recall_train}')\n",
    "print(f'Suppor Vector Machine Test Recall: {svc_recall_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.9 Gradiant Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "gboost = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "# Set parameters for grid search\n",
    "gboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [.08, .1, .12]\n",
    "}\n",
    "\n",
    "# Instantiate \n",
    "gb_gs = GridSearchCV(gboost, param_grid = gboost_params, cv = 5)\n",
    "\n",
    "# Fit\n",
    "gb_gs.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Determine best parameters\n",
    "print(f'Best parameters: {gb_gs.best_params_}')\n",
    "print('')\n",
    "\n",
    "# Set best model to a new variable\n",
    "gb_model = gb_gs.best_estimator_\n",
    "\n",
    "# Score model\n",
    "print(f'Accuracy Score on training set: {gb_model.score(X_train_new, y_train_new)}')\n",
    "print(f'Accuracy Score on testing set: {gb_model.score(X_test_sc, y_test)}')\n",
    "\n",
    "# predicting values for linear regression\n",
    "y_hat_gb_train = gb_model.predict(X_train_new)\n",
    "y_hat_gb_test = gb_model.predict(X_test_sc)\n",
    "\n",
    "# checking recall scores\n",
    "gb_recall_train = recall_score(y_train_new, y_hat_gb_train)\n",
    "gb_recall_test = recall_score(y_test, y_hat_gb_test)\n",
    "\n",
    "print(f'Suppor Vector Machine Train Recall: {gb_recall_train}')\n",
    "print(f'Suppor Vector Machine Test Recall: {gb_recall_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
